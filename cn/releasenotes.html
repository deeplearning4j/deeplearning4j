---
title: Release Notes
layout: cn-default
---
<p><h1>Deeplearning4j 最新更新</h1></p><br>
<p><h2>Release Notes for Version 0.9.1</h2>
<p><strong>Deeplearning4J</strong></p>
<ul>
  <li>Fixed issue with incorrect version dependencies in 0.9.0</li>
  <li>Added EmnistDataSetIterator <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-core/src/main/java/org/deeplearning4j/datasets/iterator/impl/EmnistDataSetIterator.java#L32">Link</a></li>
  <li>Numerical stability improvements to LossMCXENT / LossNegativeLogLikelihood with softmax (should reduce NaNs with very large activations)</li>
</ul>
<p><strong>ND4J</strong></p>
<ul>
  <li>Added runtime version checking for ND4J, DL4J, RL4J, Arbiter, DataVec <a href="https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/versioncheck/VersionCheck.java">Link</a></li>
</ul>
<p><strong>Known Issues</strong></p>
<ul>
  <li>Deeplearning4j: Use of Evaluation class no-arg constructor (i.e., new Evaluation()) can result in accuracy/stats being reported as 0.0. Other Evaluation class constructors, and ComputationGraph/MultiLayerNetwork.evaluate(DataSetIterator) methods work as expected.</li>
</ul>
<br>
<br>
<p><h2>Release Notes for Version 0.9.0</h2>
<p><strong>Deeplearning4J</strong></p>
<ul>
  <li>Workspaces feature added (faster training performance + less memory) <a href="https://deeplearning4j.org/workspaces">Link</a></li>
  <li>SharedTrainingMaster added for Spark network training (improved performance) <a href="https://deeplearning4j.org/distributed">Link 1</a>, <a href="https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-spark-examples/dl4j-spark/src/main/java/org/deeplearning4j/mlp/MnistMLPDistributedExample.java">Link 2</a></li>
  <li>ParallelInference added - wrapper that server inference requests using internal batching and queues <a href="https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/inference/ParallelInferenceExample.java">Link</a></li>
  <li>ParallelWrapper now able to work with gradients sharing, in addition to existing parameters averaging mode <a href="https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-cuda-specific-examples/src/main/java/org/deeplearning4j/examples/multigpu/GradientsSharingLenetMnistExample.java">Link</a></li>
  <li>VPTree performance significantly improved</li>
  <li>CacheMode network configuration option added - improved CNN and LSTM performance at the expense of additional memory use <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/CacheMode.java">Link</a></li>
  <li>LSTM layer added, with CuDNN support <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/LSTM.java">Link</a> (Note that the existing GravesLSTM implementation does not support CuDNN)</li>
  <li>New native model zoo with pretrained ImageNet, MNIST, and VGG-Face weights <a href="https://github.com/deeplearning4j/deeplearning4j/tree/master/deeplearning4j-zoo/src/main/java/org/deeplearning4j/zoo">Link</a></li>
  <li>Convolution performance improvements, including activation caching</li>
  <li>Custom/user defined updaters are now supported <a href="https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/learning/config/IUpdater.java">Link</a></li>
  <li>Evaluation improvements
    <ul>
      <li>EvaluationBinary, ROCBinary classes added: for evaluation of binary multi-class networks (sigmoid + xent output layers) <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/eval/EvaluationBinary.java">Link</a></li>
      <li>Evaluation and others now have G-Measure and Matthews Correlation Coefficient support; also macro + micro-averaging support for Evaluation class metrics <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/eval/EvaluationAveraging.java">Link</a></li>
      <li>ComputationGraph and SparkComputationGraph evaluation convenience methods added (evaluateROC, etc)</li>
      <li>ROC and ROCMultiClass support exact calculation (previous: thresholded calculation was used) <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/eval/ROC.java#L78-L81">Link</a></li>
      <li>ROC classes now support area under precision-recall curve calculation; getting precision/recall/confusion matrix at specified thresholds (via PrecisionRecallCurve class) <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/eval/curves/PrecisionRecallCurve.java#L102-L193">Link</a></li>
      <li>RegressionEvaluation, ROCBinary etc now support per-output masking (in addition to per-example/per-time-step masking)</li>
      <li>EvaluationCalibration added (residual plots, reliability diagrams, histogram of probabilities) <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/eval/EvaluationCalibration.java">Link 1</a> <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-core/src/main/java/org/deeplearning4j/evaluation/EvaluationTools.java#L180-L188">Link 2</a></li>
      <li>Evaluation and EvaluationBinary: now supports custom classification threshold or cost array <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/eval/Evaluation.java#L150-L170">Link</a></li>
    </ul>
  </li>
  <li>Optimizations: updaters, bias calculation</li>
  <li>Network memory estimation functionality added. Memory requirements can be estimated from configuration without instantiating networks <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/MultiLayerConfiguration.java#L310-L317">Link 1</a> <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/ComputationGraphConfiguration.java#L451-L458">Link 2</a></li>
  <li>New loss functions:
    <ul>
      <li>Mixture density loss function <a href="https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/lossfunctions/impl/LossMixtureDensity.java">Link</a></li>
      <li>F-Measure loss function <a href="https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/lossfunctions/impl/LossFMeasure.java">Link</a></li>
    </ul>
  </li>
</ul>
<p><strong>ND4J</strong></p>
<ul>
  <li>Workspaces feature added <a href="https://github.com/deeplearning4j/dl4j-examples/blob/master/nd4j-examples/src/main/java/org/nd4j/examples/Nd4jEx15_Workspaces.java">Link</a></li>
  <li>Native parallel sort was added</li>
  <li>New ops added: SELU/SELUDerivative, TAD-based comparisons, percentile/median, Reverse, Tan/TanDerivative, SinH, CosH, Entropy, ShannonEntropy, LogEntropy, AbsoluteMin/AbsoluteMax/AbsoluteSum, Atan2</li>
  <li>New distance functions added: CosineDistance, HammingDistance, JaccardDistance</li>
</ul>
<p><strong>DataVec</strong></p>
<ul>
  <li>MapFileRecordReader and MapFileSequenceRecordReader added <a href="https://github.com/deeplearning4j/DataVec/blob/master/datavec-hadoop/src/main/java/org/datavec/hadoop/records/reader/mapfile/MapFileRecordReader.java">Link 1</a> <a href="https://github.com/deeplearning4j/DataVec/blob/master/datavec-hadoop/src/main/java/org/datavec/hadoop/records/reader/mapfile/MapFileSequenceRecordReader.java">Link 2</a></li>
  <li>Spark: Utilities to save and load JavaRDD&lt;List&lt;Writable&gt;&gt; and JavaRDD&lt;List&lt;List&lt;Writable&gt;&gt; data to Hadoop MapFile and SequenceFile formats <a href="https://github.com/deeplearning4j/DataVec/blob/master/datavec-spark/src/main/java/org/datavec/spark/storage/SparkStorageUtils.java">Link</a></li>
  <li>TransformProcess and Transforms now support NDArrayWritables and NDArrayWritable columns</li>
  <li>Multiple new Transform classes</li>
</ul>
<p><strong>Arbiter</strong></p>
<ul>
  <li>Arbiter UI: <a href="https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/arbiter/BasicHyperparameterOptimizationExample.java">Link</a>
    <ul>
      <li>UI now uses Play framework, integrates with DL4J UI (replaces Dropwizard backend). Dependency issues/clashing versions fixed.</li>
      <li>Supports DL4J StatsStorage and StatsStorageRouter mechanisms (FileStatsStorage, Remote UI via RemoveUIStatsStorageRouter)</li>
      <li>General UI improvements (additional information, formatting fixes)</li>
    </ul>
  </li>
</ul>
<br>
<br>
<p><h2>Release Notes for Version 0.8.0</h2>
<ul>
  <li>Added transfer learning API <a href="https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/transferlearning/vgg16">Link</a></li>
  <li>Spark 2.0 support (DL4J and DataVec; see transition notes below)</li>
  <li>New layers
    <ul>
      <li>Global pooling (aka &ldquo;pooling over time&rdquo;; usable with both RNNs and CNNs) <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/layers/pooling/GlobalPoolingLayer.java">Link</a></li>
      <li>Center loss output layer <a href="https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/misc/centerloss/CenterLossLenetMnistExample.java">Link</a></li>
      <li>1D Convolution and subsampling layers <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/layers/convolution/Convolution1DLayer.java">Link</a> <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/layers/convolution/subsampling/Subsampling1DLayer.java">Link2</a></li>
      <li>ZeroPaddingLayer <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/layers/convolution/ZeroPaddingLayer.java">Link</a></li>
    </ul>
  </li>
  <li>New ComputationGraph vertices
    <ul>
      <li>L2 distance vertex</li>
      <li>L2 normalization vertex</li>
    </ul>
  </li>
  <li>Per-output masking is now supported for most loss functions (for per output masking, use a mask array equal in size/shape to the labels array; previous masking functionality was per-example for RNNs)</li>
  <li>L1 and L2 regularization can now be configured for biases (via l1Bias and l2Bias configuration options)</li>
  <li>Evaluation improvements:
    <ul>
      <li>DL4J now has an IEvaluation class (that Evaluation, RegressionEvaluation, etc all implement. Also allows custom evaluation on Spark) <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/eval/IEvaluation.java">Link</a></li>
      <li>Added multi-class (one vs. all) ROC: ROCMultiClass <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/eval/ROCMultiClass.java">Link</a></li>
      <li>For both MultiLayerNetwork and SparkDl4jMultiLayer: added evaluateRegression, evaluateROC, evaluateROCMultiClass convenience methods</li>
      <li>HTML export functionality added for ROC charts <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-core/src/main/java/org/deeplearning4j/evaluation/EvaluationTools.java#L93">Link</a></li>
      <li>TSNE re-added to new UI</li>
      <li>Training UI: now usable without an internet connection (no longer relies on externally hosted fonts)</li>
      <li>UI: improvements to error handling for &lsquo;no data&rsquo; condition</li>
    </ul>
  </li>
  <li>Epsilon configuration now used for Adam and RMSProp updaters</li>
  <li>Fix for bidirectional LSTMs + variable-length time series (using masking)</li>
  <li>Added CnnSentenceDataSetIterator (for use with &lsquo;CNN for Sentence Classification&rsquo; architecture) <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/iterator/CnnSentenceDataSetIterator.java">Link</a> <a href="https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/convolution/sentenceClassification/CnnSentenceClassificationExample.java">Link2</a></li>
  <li>Spark + Kryo: now test serialization + throw exception if misconfigured (instead of logging an error that can be missed)</li>
  <li>MultiLayerNetwork now adds default layer names if no name is specified</li>
  <li>DataVec:
    <ul>
      <li>JSON/YAML support for DataAnalysis, custom Transforms etc</li>
      <li>ImageRecordReader refactored to reduce garbage collection load (hence improve performance with large training sets)</li>
      <li>Faster quality analysis.</li>
    </ul>
  </li>
  <li>Arbiter: added new layer types to match DL4J
    <ul>
      <li>Performance improvement for Word2Vec/ParagraphVectors tokenization &amp; training.</li>
    </ul>
  </li>
  <li>Batched inference introduced for ParagraphVectors</li>
  <li>Nd4j improvements
    <ul>
      <li>New native operations available for ND4j: firstIndex, lastIndex, remainder, fmod, or, and, xor.</li>
      <li>OpProfiler NAN_PANIC &amp; INF_PANIC now also checks result of BLAS calls.</li>
      <li>Nd4.getMemoryManager() now provides methods to tweak GC behavior.</li>
    </ul>
  </li>
  <li>Alpha version of parameter server for Word2Vec/ParagraphVectors were introduced for Spark. Please note: It&rsquo;s not recommended for production use yet.</li>
  <li>Performance improvements for CNN inference</li>
</ul><br>
<br>
<h3>0.7.2 -> 0.8.0 Transition Notes</h3>
<p>Spark versioning schemes: with the addition of Spark 2 support, the versions for Deeplearning4j and DataVec Spark modules has changed</p>
<ul>
	<li>For Spark 1: use <code>&lt;version&gt;0.8.0_spark_1&lt;/version&gt;</code></li>
	<li>For Spark 2: use <code>&lt;version&gt;0.8.0_spark_2&lt;/version&gt;</code></li>
  <li>Also note: Modules with Spark 2 support are released with Scala 2.11 support only. Spark 1 modules are released with both Scala 2.10 and 2.11 support</li>
</ul><br>
<br>
<h3>0.8.0 Known Issues (At Launch)</h3>
<ul>
  <li>UI/CUDA/Linux issue: <a href="https://github.com/deeplearning4j/deeplearning4j/issues/3026">Link</a></li>
  <li>Dirty shutdown on JVM exit is possible for CUDA backend sometimes: <a href="https://github.com/deeplearning4j/deeplearning4j/issues/3028">Link</a></li>
  <li>Issues with RBM implementation <a href="https://github.com/deeplearning4j/deeplearning4j/issues/3049">Link</a></li>
</ul>
<br>
<br>

<p><h2>Release Notes for Version 0.7.2</h2>
<ul>
  <li>Added variational autoencoder <a href="https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/unsupervised/variational/VariationalAutoEncoderExample.java">Link</a></li>
  <li>Activation function refactor
    <ul>
      <li>Activation functions are now an interface <a href="https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/activations/IActivation.java">Link</a></li>
      <li>Configuration now via enumeration, not via String (see examples - <a href="https://github.com/deeplearning4j/dl4j-examples">Link</a>)</li>
      <li>Custom activation functions now supported <a href="https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/misc/activationfunctions/CustomActivationExample.java">Link</a></li>
      <li>New activation functions added: hard sigmoid, randomized leaky rectified linear units (RReLU)</li>
    </ul>
  </li>
  <li>Multiple fixes/improvements for Keras model import</li>
  <li>Added P-norm pooling for CNNs (option as part of SubsamplingLayer configuration)</li>
  <li>Iteration count persistence: stored/persisted properly in model configuration + fixes to learning rate schedules for Spark network training</li>
  <li>LSTM: gate activation function can now be configured (previously: hard-coded to sigmoid)</li>
  <li>UI:
    <ul>
      <li>Added Chinese translation</li>
      <li>Fixes for UI + pretrain layers</li>
      <li>Added Java 7 compatible stats collection compatibility <a href="https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/userInterface/UIStorageExample_Java7.java">Link</a></li>
      <li>Improvements in front-end for handling NaNs</li>
      <li>Added UIServer.stop() method</li>
      <li>Fixed score vs. iteration moving average line (with subsampling)</li>
    </ul>
  </li>
  <li>Solved Jaxb/Jackson issue with Spring Boot based applications</li>
  <li>RecordReaderDataSetIterator now supports NDArrayWritable for the labels (set regression == true; used for multi-label classification + images, etc)</li>
</ul><br>

<p><h3>0.7.1 -> 0.7.2 Transition Notes</h3></p>
<p>Activation functions (built-in): now specified using Activation enumeration, not String (String-based configuration has been deprecated)</p><br>
<p><h2>Release Notes for Version 0.7.1</h2></p>
<ul>
  <li>RBM and AutoEncoder key fixes:
    <ul>
      <li>Ensured visual bias updated and applied during pretraining.</li>
      <li>RBM HiddenUnit is the activation function for this layer; thus, established derivative calculations for backprop according to respective HiddenUnit.</li>
    </ul>
  </li>
  <li>RNG performance issues fixed for CUDA backend</li>
  <li>OpenBLAS issues fixed for macOS, powerpc, linux.</li>
  <li>DataVec is back to Java 7 now.</li>
  <li>Multiple minor bugs fixed for ND4J/DL4J</li>
</ul><br>

<p><h2>Release Notes for Version 0.7.0</h2></p>
<ul>
  <li>UI overhaul: new training UI has considerably more information, supports persistence (saving info and loading later), Japanese/Korean/Russian support. Replaced Dropwizard with Play framework. <a href="https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/userInterface">Link</a></li>
  <li>Import of models configured and trained using <a href="http://keras.io/">Keras</a>
    <ul>
      <li>Imports both <em>Keras</em> model <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/ModelConfiguration.java">configurations</a> and <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/Model.java#L59">stored weights</a></li>
      <li>Supported models: <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/ModelConfiguration.java#L41">Sequential</a> models</li>
      <li>Supported <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/LayerConfiguration.java#L85">layers</a>: <em>Dense, Dropout, Activation, Convolution2D, MaxPooling2D, LSTM</em></li>
    </ul>
  </li>
  <li>Added &lsquo;Same&rsquo; padding more for CNNs (ConvolutionMode network configuration option) <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/ConvolutionMode.java">Link</a></li>
  <li>Weighted loss functions: Loss functions now support a per-output weight array (row vector)</li>
  <li>ROC and AUC added for binary classifiers <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/eval/ROC.java">Link</a></li>
  <li>Improved error messages on invalid configuration or data; improved validation on both</li>
  <li>Added metadata functionality: track source of data (file, line number, etc) from data import to evaluation. Loading a subset of examples/data from this metadata is now supported. <a href="https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/dataExamples/CSVExampleEvaluationMetaData.java">Link</a></li>
  <li>Removed Jackson as core dependency (shaded); users can now use any version of Jackson without issue</li>
  <li>Added LossLayer: version of OutputLayer that only applies loss function (unlike OutputLayer: it has no weights/biases)</li>
  <li>Functionality required to build triplet embedding model (L2 vertex, LossLayer, Stack/Unstack vertices etc)</li>
  <li>Reduced DL4J and ND4J &lsquo;cold start&rsquo; initialization/start-up time</li>
  <li>Pretrain default changed to false and backprop default changed to true. No longer needed to set these when setting up a network configuration unless defaults need to be changed.</li>
  <li>Added TrainingListener interface (extends IterationListener). Provides access to more information/state as network training occurs <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/optimize/api/TrainingListener.java">Link</a></li>
  <li>Numerous bug fixes across DL4J and ND4J</li>
  <li>Performance improvements for nd4j-native &amp; nd4j-cuda backends</li>
  <li>Standalone Word2Vec/ParagraphVectors overhaul:
    <ul>
      <li>Performance improvements</li>
      <li>ParaVec inference available for both PV-DM &amp; PV-DBOW</li>
      <li>Parallel tokenization support was added, to address computation-heavy tokenizers.</li>
    </ul>
  </li>
  <li>Native RNG introduced for better reproducibility within multi-threaded execution environment.</li>
  <li>Additional RNG calls added: Nd4j.choice(), and BernoulliDistribution op.</li>
  <li>Off-gpu storage introduced, to keep large things, like Word2Vec model in host memory. Available via WordVectorSerializer.loadStaticModel()</li>
  <li>Two new options for performance tuning on nd4j-native backend: setTADThreshold(int) &amp; setElementThreshold(int)</li>
</ul>
<br>
<p><h3>0.6.0 -> 0.7.0 Transition Notes</h3></p>
<p>Notable changes for upgrading codebases based on 0.6.0 to 0.7.0:</p>
<ul>
  <li>UI: new UI package name is deeplearning4j-ui_2.10 or deeplearning4j-ui_2.11 (previously: deeplearning4j-ui). Scala version suffix is necessary due to Play framework (written in Scala) being used now.</li>
  <li>Histogram and Flow iteration listeners deprecated. They are still functional, but using new UI is recommended <a href="https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/userInterface">Link</a></li>
  <li>DataVec ImageRecordReader: labels are now sorted alphabetically by default before assigning an integer class index to each - previously (0.6.0 and earlier) they were according to file iteration order. Use .setLabels(List) to manually specify the order if required.</li>
  <li>CNNs: configuration validation is now less strict. With new ConvolutionMode option, 0.6.0 was equivalent to &lsquo;Strict&rsquo; mode, but new default is &lsquo;Truncate&rsquo;
    <ul>
      <li>See ConvolutionMode javadoc for more details: <a href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/ConvolutionMode.java">Link</a></li>
    </ul>
  </li>
  <li>Xavier weight initialization change for CNNs and LSTMs: Xavier now aligns better with original Glorot paper and other libraries. Xavier weight init. equivalent to 0.6.0 is available as XAVIER_LEGACY</li>
  <li>DataVec: Custom RecordReader and SequenceRecordReader classes require additional methods, for the new metadata functionality. Refer to existing record reader implementations for how to implement these methods.</li>
  <li>Word2Vec/ParagraphVectors:
    <ul>
      <li>Few new builder methods:
        <ul>
          <li>allowParallelTokenization(boolean)</li>
          <li>useHierarchicSoftmax(boolean)</li>
        </ul>
      </li>
      <li>Behaviour change: batchSize: now batch size is ALSO used as threshold to execute number of computational batches for sg/cbow</li>
    </ul>
  </li>
</ul>
<p><h2>Release Notes for Version 0.6.0</h2></p>
<ul>
  <li>Custom layer support</li>
  <li>Support for custom loss functions</li>
  <li>Support for compressed INDArrays, for memory saving on huge data</li>
  <li>Native support for BooleanIndexing where applicable</li>
  <li>Initial support for combined operations on CUDA</li>
  <li>Significant performance improvements on CPU &amp; CUDA backends</li>
  <li>Better support for Spark environments using CUDA &amp; cuDNN with multi-gpu clusters</li>
  <li>New UI tools: FlowIterationListener and ConvolutionIterationListener, for better insights of processes within NN.</li>
  <li>Special IterationListener implementation for performance tracking: PerformanceListener</li>
  <li>Inference implementation added for ParagraphVectors, together with option to use existing Word2Vec model</li>
  <li>Severely decreased file size on the deeplearnning4j api</li>
  <li>nd4j-cuda-8.0 backend is available now for cuda 8 RC</li>
  <li>Added multiple new built-in loss functions</li>
  <li>Custom preprocessor support</li>
  <li>Performance improvements to Spark training implementation</li>
  <li>Improved network configuration validation using InputType functionality</li>
</ul><br>

<p><h2>Release Notes for Version 0.5.0</h2></p>
<ul>
  <li>FP16 support for CUDA</li>
  <li>[Better performance for multi-gpu}(http://deeplearning4j.org/gpu)</li>
  <li>Including optional P2P memory access support</li>
  <li>Normalization support for time series and images</li>
  <li>Normalization support for labels</li>
  <li>Removal of Canova and shift to DataVec: <a href="http://deeplearning4j.org/datavecdoc/">Javadoc</a>, <a href="https://github.com/deeplearning4j/datavec">Github Repo</a></li>
  <li>Numerous bug fixes</li>
  <li>Spark improvements</li>
</ul><br>

<p><h2>Release Notes for version 0.4.0</h2></p>
<ul>
  <li>Initial multi-GPU support viable for standalone and Spark.</li>
  <li>Refactored the Spark API significantly</li>
  <li>Added CuDNN wrapper</li>
  <li>Performance improvements for ND4J</li>
  <li>Introducing <a href="https://github.com/deeplearning4j/datavec">DataVec</a>: Lots of new functionality for transforming, preprocessing, cleaning data. (This replaces Canova)</li>
  <li>New DataSetIterators for feeding neural nets with existing data: ExistingDataSetIterator, Floats(Double)DataSetIterator, IteratorDataSetIterator</li>
  <li>New learning algorithms for word2vec and paravec: CBOW and PV-DM respectively</li>
  <li>New native ops for better performance: DropOut, DropOutInverted, CompareAndSet, ReplaceNaNs</li>
  <li>Shadow asynchronous datasets prefetch enabled by default for both MultiLayerNetwork and ComputationGraph</li>
  <li>Better memory handling with JVM GC and CUDA backend, resulting in significantly lower memory footprint</li>
</ul><br>

<p><h2>资源</h2></p>
<ul>
  <li><a href="https://search.maven.org/#search%7Cga%7C1%7Cdeeplearning4j">Deeplearning4j on Maven Central</a></li>
  <li><a href="https://github.com/deeplearning4j/deeplearning4j/">Deeplearning4j Source Code</a></li>
  <li><a href="https://github.com/deeplearning4j/nd4j/">ND4J Source Code</a></li>
  <li><a href="https://github.com/deeplearning4j/libnd4j/">libnd4j Source Code</a></li>
</ul><br>

<p><h2>Roadmap for Fall 2016</h2></p>
<ul>
  <li><a href="https://github.com/deeplearning4j/scalnet">ScalNet Scala API</a> (WIP!)</li>
  <li>Standard NN configuration file shared with Keras</li>
  <li>CGANs</li>
  <li>Model interpretability</li>
</ul>
<p>&nbsp;</p>