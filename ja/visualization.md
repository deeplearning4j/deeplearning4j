- - -
title：ネットワーク学習の視覚化、監視、デバッグ
layout： default
---

# ネットワーク学習の視覚化、監視、デバッグ 

目次

* [Deeplearning4jのトレーニングUIでネットワークトレーニングを視覚化](#ui)
    * [Deeplearning4jのUI：概略ページ](#overviewpage)
    * [Deeplearning4jのUI：モデルのページ](#modelpage)
* [Deeplearning4JのUIとSparkのトレーニング](#sparkui)
* [UIを使用してネットワークを調節](#usingui)
* [t-SNEとWord2Vecとは？](#tsne)

## <a name="ui">Deeplearning4jのトレーニングUIでネットワークトレーニングを視覚化</a>

**備考**：この情報はDL4Jのバージョン0.7.0及びそれ以降に関連しています。

DL4Jは、ユーザーインターフェースを提供することによって、ブラウザに（リアルタイムで）現在のネットワークの状況とトレーニングの進捗を視覚化します。UIは一般にニューラルネットワークの調整をサポートするために使用されます。すなわち、ネットワークでのよい性能を達成するためのハイパーパラメータの選択（学習速度など）です。

**第1段階：Deeplearning4jのUI依存関係を対象とするプロジェクトに追加してください。**

```
    <dependency>
        <groupId>org.deeplearning4j</groupId>
        <artifactId>deeplearning4j-ui_2.10</artifactId>
        <version>${dl4j.version}</version>
    </dependency>
```

拡張子の```_2.10```はScalaのバージョンであることにご注意ください（バックエンドにScalaライブラリであるPlayフレームワークを使用しているため）その他のScalaライブラリを使用していない場合は、```_2.10```や```_2.11```でいいでしょう。

**第2段階：プロジェクト内のUIを有効化してください。**

これは比較的簡単です。

```
    //ユーザーインターフェースのバックエンドを初期化する
    UIServer uiServer = UIServer.getInstance();

    //ネットワーク情報（勾配、スコア対時間など）が保管される場所の情報を設定するこちらのメモリーに保管します。
    StatsStorage statsStorage = new InMemoryStatsStorage();         //その他の選択肢： 新しいFileStatsStorage(File)。これは後で保存、ロードするため。
    
    //StatsStorageインスタンスをUIに付ける。これによりStatsStorageのコンテンツが視覚化される。
    uiServer.attach(statsStorage);

    //次に、トレーニングしながらネットワークからこの情報を収集するためにStatsListenerを追加する。
    net.setListeners(new StatsListener(statsStorage));
```

UIにアクセスするためにブラウザを開き、```http：//localhost：9000/train```へ行く。
ポートは、 ```org.deeplearning4j.ui.port```のシステムプロパティを使用することによって設定できます。つまり、ポートの9001を使用するには、開始時に以下をJVMに渡します。```-Dorg.deeplearning4j.ui.port=9001```

それから、情報は収集され、ネットワークに```fit```メソッドを呼び出した時にUIに送られます。 


**例：**UIの例は[こちら](https：//github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/userInterface/UIExample.java)をご覧ください。

UIの一通りの例は[こちら](https：//github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/userInterface)をご覧ください。


### <a name="overviewpage">Deeplearning4jのUI：概略ページ</a>

![Overview Page](./img/DL4J_UI_01.png)

概略ページ（3ページ中の1ページ）には次の情報が含まれてます。

- 左上：スコア対イテレーションのチャート - これは現在のミニバッチの損失関数
- 右上：モデルとトレーニングの情報
- 左下：すべてのネットワークの重量対イテレーションについて、パラメータ対アップデート（層ごと）の比率
- 右下：活性化、勾配、アップデートの標準偏差（対時間）

なお、下方のチャート二つは、値の対数（10を底とする）で表示されています。従って、アップデート対パラメータの比率チャートにおける-3という値は10<sup>-3</sup> = 0.001という比率に相当します。

つまり、パラメータ対アップデートの比率はこれらの値の平均的大きさの比率です。

このページにある後のセクションでこれらの値の使用方法をお読みください。

### <a name="modelpage">Deeplearning4jのUI：モデルのページ</a>

![Model Page](./img/DL4J_UI_02.png)

モデルのページにはニューラルネットワークの層のグラフが含まれています。このグラフは選択メカニズムとしての働きをしています。層をクリックするとこの情報を見ることができます。

層を選択すると右側に次のようなチャートが表示されます。

- 層についての情報を表にしたもの
- 概略ページにより、その層のアップデート対パラメータの比率この比率のコンポーネント（パラメータとアップデートの平均的大きさ）はタブでも見ることができます。
- 時間の経過とともに層の活性化（平均と平均の標準偏差+/- 2）
- 各種類のパラメータとアップグレードのヒストグラムとアップデート
- 学習速度対時間（学習速度のスケジュールが使用されない限り、平らになるでしょう。）


備考：パラメータは重みは(W)、バイアスは(b)としています。再帰型ニューラルネットワークは、Wは二層間を接続するもの、RWはの再帰重みを指します。（タイムステップの間の重み）




## <a name="sparkui">Deeplearning4JのUIとSparkのトレーニング</a>

DL4JのUIはSparkで使用することができます。しかし、0.7.0以降、依存関係の衝突により、UIとSparkを同じJVMで実行するのは困難である可能性が出てきました。

代替策として二つの方法があります。

1.後に（オフラインで）視覚化するために適切な統計結果を収集、保存しておく。
2.UIを別のサーバで実行し、リモートUIの機能を使ってSpark MasterからUIインスタンスへのデータをアップロードする。

**後にオフラインで使用するために統計結果を収集**

```
    SparkDl4jMultiLayer sparkNet = new SparkDl4jMultiLayer(sc, conf, tm);
    
    StatsStorage ss = new FileStatsStorage(new File("myNetworkTrainingStats.dl4j"));
    sparkNet.setListeners(ss, Collections.singletonList(new StatsListener(null)));
```

そして、以下を使用してあとで保存しておいた情報を読み込み、表示することができます。

```
    StatsStorage statsStorage = new FileStatsStorage(statsFile);    //もしファイルが既に存在する場合。データをそこから読み込みます。
    UIServer uiServer = UIServer.getInstance();
    uiServer.attach(statsStorage);
```

**リモートUIの機能を使用する**

まず、UIを実行しているJVMで以下を実行します。

```
    UIServer uiServer = UIServer.getInstance();
    uiServer.enableRemoteListener();        //必要なリモートサポートはデフォルトで有効化されていません。
```
これには、```deeplearning4j-ui_2.10```または```deeplearning4j-ui_2.11```の存関係が必要です。

次に、Sparkのトレーニングのインスタンスに以下を実行します。

```
    SparkDl4jMultiLayer sparkNet = new SparkDl4jMultiLayer(sc, conf, tm);

    StatsStorageRouter remoteUIRouter = new RemoteUIStatsStorageRouter("http：//UI_MACHINE_IP：9000");
    sparkNet.setListeners(remoteUIRouter, Collections.singletonList(new StatsListener(null)));
```
Sparkとの依存関係の衝突を回避するには、フルのUIの依存関係```deeplearning4j-ui_2.10```でなく```deeplearning4j-ui-model```を使用してStatsListenerを得る必要があります。

備考： ```UI_MACHINE_IP```をユーザーインターフェイスを実行しているマシンのIPアドレスに変えなければなりません。




## <a name="usingui">UIを使用してネットワークを調節</a>

ニューラルネットワークのトレーニングの視覚化については、Andrej Karpathy氏による[こちら](http：//cs231n.github.io/neural-networks-3/#baby)のウェブサイトがおすすめです。まず最初にこのサイトを読み、理解を深めるといいでしょう。

ニューラルネットワークの調節は、科学というより芸術の領域です。とはいえ、ここで役に立つかもしれない情報をご紹介しましょう。

**概略ページ - モデルの値 vs. イテレーションのチャート**

値対イテレーションは、時間の経過とともに（全体的に）小さくなってくるはずです。

値が一貫して増加している場合、学習率のが高く設定されて過ぎている可能性があります。値がより安定するまで値を低下させてみてください。
- また、値の増加は、不正確なデータの正規化など、その他のネットワーク問題が関連している可能性があります。
- 値が変わらないままである、または非常にゆっくり減少している（数百のイテレーションにつき）場合、(a) 学習率が低過ぎる、または(b) 最適化に困難が生じる可能性があります。後者の場合、確率的勾配降下法を使用している場合は、Nesterovs（慣性）、RMSProp、Adagradなどの別のアップデーターを使ってみてください。
十分にシャッフルされていないデータ（つまり、各ミニバッチに分類のクラスが1つしかない）だと、値対イテレーションのグラフが非常に大まかな、または正常そうでない結果になる可能性があります。
- この折れ線グラフにノイズが発生することは予測されます（つまり、折れ線は小さな範囲内で上下するでしょう）。しかし、値が前後の実行で著しく異なる場合、バリエーションが非常に大きく、問題になる可能性があります。
    - 先に挙げた問題（学習率、正常化、データシャフリング）が関連している可能性があります。
    - ミニバッチのサイズを非常に少数のサンプルに設定すると、これもノイズの多い値対イテレーションのグラフに関連し、最適化に困難が生じる*かもしれません* 。

**概略ページとモデルのページ - アップデードの使用：パラメータ比のグラフ**

- パラメーターの平均的大きさ対更新の比率は、概略ページとモデルページの両方に提供されています。
    - "「平均的大きさ」 = パラメータの絶対値の平均や現在のタイムステップの更新
- この比率の最も重要な使用法は学習率の選択です。大まかにいうとこの比率は1：1000 = 0.001になります。従って、(log<sub>10</sub>)のグラフだと、-3になります。（つまり、10<sup>-3</sup> = 0.001）
    - これは大まかな目安であって、すべてのネットワークにおいて当てはまるとは限りません。しかし、ここから開始するのは妥当でしょう。
    - もし比率がこの値とかけ離れている場合、（例えば、> -2(つまり、10<sup>-2</sup>=0.01）または、< -4 （つまり、10<sup>-4</sup>=0.0001）お使いのパラメータは有益な特徴を学習するには不安定過ぎる、または変化の速度が遅すぎる可能性があります。
    - この比率を変えるには、学習率を調節してください（また、パレメーター初期化をするのがいいこともあります）。層によって異なる学習率を設定するのがいいネットワークもあります。
- 異常なほどの比率の急上昇がないか注意してください。勾配爆発が発生している可能性があります。


**モデルのページ：層の活性化（対時間）グラフ**

このグラフは消失または爆発する活性化を発見するのに使用できます（原因は重み初期化に問題があり、過度の正規化、学習率が高すぎるなど）。

- 何も問題がなければ、このグラフは時間の経過（通常は数百回のイテレーション）とともに安定化してくるはずです。
- 活性化に望ましい標準偏差は、約0.5から2.0です。この範囲からかけ離れている場合は、先に挙げた問題が発生している可能性があります。

**モデルのページ：層パラメータのヒストグラム**

層パレメータのヒストグラムは最も最近ではイテレーション用のみに表示されます。

- 重みについては、これらのヒストグラムはしばらく時間が経つとガウス（正規）分布の形状になっていきます。
- バイアスに関しては、これらのヒストグラムは一般に0から開始し、ガウス分布の形状になっていくでしょう。
    - これに関する一つの例外は、長・短期記憶の再帰型ニューラルネットワークの層です。デフォルト設定では、ゲート一つのバイアスは1.0（デフォルト設定。ただし、変更可能です。）に設定されています。これは依存関係を長期に渡って学習するためです。これによって、バイアスのグラフには最初は約0.0のバイアスと約1.0のバイアスが多くあるでしょう。
- パラメータが+/-無限の状態になっているパラメータに注意してください。学習率が高すぎたり、正規化が十分でないとこのような状態になります（L2正規化を追加することをおすすめします）。
- バイアスが大きくなり過ぎないか注意してください。クラスの分布にバランスが取れていないと、出力層でこの問題が発生することはあります。

**モデルのページ：層アップデートのヒストグラム**

層アップデートのヒストグラムは最も最近のイテレーションのみを表示します。

- これらはアップデートです。つまり、学習率、慣性項、正規化を適用した*後の*勾配です。
- パラメータについては、これらのヒストグラムはしばらく時間が経つとガウス（正規）分布の形状になっていきます。
- 異常なほど大きな値がないか注意してください。ネットワークに勾配爆発が発生している可能性があります。
    勾配爆発はネットワークのパラメータを「混乱させる」ため、問題です。
    - この場合、重み初期化、学習率、入力／ラベルデータ正規化の問題が発生しているかもしれません。
    - 再帰型ニューラルネットワークの場合、 [勾配の正規化や勾配のクリッピング](https：//github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-core/src/main/java/org/deeplearning4j/nn/conf/GradientNormalization.java)を追加するといいかもしれません。

**モデルのページ：パラメータ学習率グラフ**

このグラフは、層パラメータの時間を追った学習率を表しています。

学習率のスケジュールを使用していない場合は、グラフが平らになるでしょう。学習率のスケジュールを使用している場合は、このグラフを使用して現在の学習率の値を時間を追って追跡することができます。


## <a name="tsne">t-SNEとWord2Vecとは？</a>

弊社では、[t-SNE](https：//lvdmaaten.github.io/tsne/)を使って[語の特徴量ベクトル](./word2vec.html)の次元を減少させ、語を2次元、または3次元空間に表示します。Word2Vecにt-SNEを使う場合のコードは以下の通りです。

        log.info("Plot TSNE....");
        BarnesHutTsne tsne = new BarnesHutTsne.Builder()
                .setMaxIter(1000)
                .stopLyingIteration(250)
                .learningRate(500)
                .useAdaGrad(false)
                .theta(0.5)
                .setMomentum(0.5)
                .normalize(true)
                .usePca(false)
                .build();
        vec.lookupTable().plotVocab(tsne);

